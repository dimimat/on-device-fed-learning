{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Under Dataset Variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, log_loss\n",
    "import seaborn as sns\n",
    "import os\n",
    "import ast\n",
    "import sys\n",
    "import re\n",
    "from statistics import mean, stdev\n",
    "statistics_path = os.path.abspath(\"../\")\n",
    "sys.path.append(statistics_path)\n",
    "import stats_utils\n",
    "from matplotlib.ticker import MaxNLocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_exp_statistics = \"/Users/admin/Desktop/thesis/dataset/metrics/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_exp_images = \"/Users/admin/Desktop/thesis_writing/experiment_images/performance_exp/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Compute Accuracy Plot for every dataset in a single diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset 1 logs\n",
    "exp_name = \"dataset_1_iid_bal_5cl_20r_1\"\n",
    "df1_try_1 = stats_utils.parse_experiments_statistics_to_df(path_to_exp_statistics, exp_name, csv_filename=\"logs.csv\")\n",
    "\n",
    "# read dataset 2 logs\n",
    "exp_name = \"dataset_2_iid_bal_5cl_20r_1\"\n",
    "df2_try_1 = stats_utils.parse_experiments_statistics_to_df(path_to_exp_statistics, exp_name, csv_filename=\"logs.csv\")\n",
    "\n",
    "# read dataset 3 logs\n",
    "exp_name = \"dataset_3_iid_bal_5cl_3\"\n",
    "df3_try_1 = stats_utils.parse_experiments_statistics_to_df(path_to_exp_statistics, exp_name, csv_filename=\"logs.csv\")\n",
    "\n",
    "# read dataset 4 logs\n",
    "exp_name = \"dataset_4_iid_bal_5cl_20r_1\"\n",
    "df4_try_1 = stats_utils.parse_experiments_statistics_to_df(path_to_exp_statistics, exp_name, csv_filename=\"logs.csv\")\n",
    "\n",
    "# read dataset 5 logs\n",
    "exp_name = \"dataset_5_iid_bal_5cl_20r_1\"\n",
    "df5_try_1 = stats_utils.parse_experiments_statistics_to_df(path_to_exp_statistics, exp_name, csv_filename=\"logs.csv\")\n",
    "\n",
    "# read dataset 6 logs\n",
    "exp_name = \"dataset_6_iid_bal_5cl_20r_1\"\n",
    "df6_try_1 = stats_utils.parse_experiments_statistics_to_df(path_to_exp_statistics, exp_name, csv_filename=\"logs.csv\")\n",
    "\n",
    "dfs = [df1_try_1, df2_try_1, df3_try_1, df4_try_1, df5_try_1, df6_try_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_loss_from_dfs(dfs, get_accuracy_loss_values, path_to_exp_images, should_save=False, filename=\"accuracies_for_every_dataset\"):\n",
    "    accs = {}\n",
    "    losses = {}\n",
    "\n",
    "    for i, df in enumerate(dfs):\n",
    "        dataset_name = f\"dataset_{i+1}\"\n",
    "        first_client_name = ast.literal_eval(df['devices_names'][0])[0]\n",
    "        acc, loss = get_accuracy_loss_values(df, first_client_name)\n",
    "        accs[dataset_name] = acc\n",
    "        losses[dataset_name] = loss\n",
    "\n",
    "    rounds = range(1, len(next(iter(accs.values()))) + 1)\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for name, values in accs.items():\n",
    "        plt.plot(rounds, values, label=name)\n",
    "    plt.title(\"Accuracy per Round\")\n",
    "    plt.xlabel(\"Federated Round\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    if should_save == False:\n",
    "        plt.show()\n",
    "    else:\n",
    "#         filename = \"accuracies_for_every_dataset\"\n",
    "        path_to_file = os.path.join(path_to_exp_images, filename)\n",
    "        plt.savefig(path_to_file, dpi=300)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_loss_from_dfs(dfs, stats_utils.get_accuracy_loss_values_for_dfs, path_to_exp_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_loss_from_dfs(\n",
    "    dfs,\n",
    "    get_accuracy_loss_values,\n",
    "    path_to_exp_images,\n",
    "    should_save=False,\n",
    "    filename=\"accuracies_for_every_dataset\",\n",
    "    label_names=None,\n",
    "    title=\"\"\n",
    "):\n",
    "    accs = {}\n",
    "    losses = {}\n",
    "\n",
    "    # 1) Extract accuracy & loss per dataset\n",
    "    for i, df in enumerate(dfs):\n",
    "        label = label_names[i]\n",
    "        first_client = ast.literal_eval(df[\"devices_names\"][0])[0]\n",
    "        acc, loss = get_accuracy_loss_values(df, first_client)\n",
    "        accs[label] = acc\n",
    "        losses[label] = loss\n",
    "\n",
    "    # 2) X-axis is 1â€¦n_rounds\n",
    "    n_rounds = len(next(iter(accs.values())))\n",
    "    rounds = list(range(1, n_rounds + 1))\n",
    "\n",
    "    # 3) Plot Accuracy\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    for label, values in accs.items():\n",
    "        ax.plot(rounds, values, label=label)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Federated Round\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "\n",
    "    # 4) Force integer ticks and explicitly include 1 and n_rounds\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.set_xticks(rounds)       # ensures the full range from 1 to n_rounds is shown\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 5) Show or save\n",
    "    if not should_save:\n",
    "        plt.show()\n",
    "    else:\n",
    "        path_to_file = os.path.join(path_to_exp_images, filename + \".png\")\n",
    "        fig.savefig(path_to_file, dpi=300)\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"accuracies_for_every_dataset\"\n",
    "title = \"Accuracy Convergence Across HAR Datasets\"\n",
    "label_names = [\"HARSense\", \"UCI Smartphone HAR\", \"Pamap2\", \"MHealth\", \"PhysioNet Acc Data\", \"MotionSense\"]\n",
    "plot_acc_loss_from_dfs(dfs, stats_utils.get_accuracy_loss_values_for_dfs, path_to_exp_images, \\\n",
    "                       should_save=True, filename=filename, label_names=label_names, title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute confusion matrices for every dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_names_list = []\n",
    "# read dataset 1 logs\n",
    "exp_name = \"dataset_1_iid_bal_5cl_20r_2\"\n",
    "df1_try_2 = stats_utils.parse_experiments_statistics_to_df(path_to_exp_statistics, exp_name, csv_filename=\"logs.csv\")\n",
    "labels_names_list.append(stats_utils.load_label_names(path_to_exp_statistics, exp_name))\n",
    "\n",
    "# read dataset 2 logs\n",
    "exp_name = \"dataset_2_iid_bal_5cl_20r_2\"\n",
    "df2_try_2 = stats_utils.parse_experiments_statistics_to_df(path_to_exp_statistics, exp_name, csv_filename=\"logs.csv\")\n",
    "labels_names_list.append(stats_utils.load_label_names(path_to_exp_statistics, exp_name))\n",
    "\n",
    "# read dataset 3 logs\n",
    "exp_name = \"dataset_3_iid_bal_5cl_3\"\n",
    "df3_try_2 = stats_utils.parse_experiments_statistics_to_df(path_to_exp_statistics, exp_name, csv_filename=\"logs.csv\")\n",
    "labels_names_list.append(stats_utils.load_label_names(path_to_exp_statistics, exp_name))\n",
    "\n",
    "# read dataset 4 logs\n",
    "exp_name = \"dataset_4_iid_bal_5cl_20r_2\"\n",
    "df4_try_2 = stats_utils.parse_experiments_statistics_to_df(path_to_exp_statistics, exp_name, csv_filename=\"logs.csv\")\n",
    "labels_names_list.append(stats_utils.load_label_names(path_to_exp_statistics, exp_name))\n",
    "\n",
    "# read dataset 5 logs\n",
    "exp_name = \"dataset_5_iid_bal_5cl_20r_2\"\n",
    "df5_try_2 = stats_utils.parse_experiments_statistics_to_df(path_to_exp_statistics, exp_name, csv_filename=\"logs.csv\")\n",
    "labels_names_list.append(stats_utils.load_label_names(path_to_exp_statistics, exp_name))\n",
    "\n",
    "# read dataset 6 logs\n",
    "exp_name = \"dataset_6_iid_bal_5cl_20r_2\"\n",
    "df6_try_2 = stats_utils.parse_experiments_statistics_to_df(path_to_exp_statistics, exp_name, csv_filename=\"logs.csv\")\n",
    "labels_names_list.append(stats_utils.load_label_names(path_to_exp_statistics, exp_name))\n",
    "\n",
    "dfs = [df1_try_2, df2_try_2, df3_try_2, df4_try_2, df5_try_2, df6_try_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_confusion_matrix_only(y_true, y_pred, label_names, path_to_exp_logs, filename, figsize=(8,6)):\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Create and configure plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", xticklabels=label_names, yticklabels=label_names)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "    # Save plot\n",
    "    path_to_file = os.path.join(path_to_exp_logs, filename)\n",
    "    plt.savefig(path_to_file, dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "for df, labels in zip(dfs, labels_names_list):\n",
    "    first_client_name = ast.literal_eval(df['devices_names'][0])[0]\n",
    "    y_true = json.loads(df['y_true'].iloc[-1])[first_client_name]\n",
    "    y_pred = json.loads(df['y_pred'].iloc[-1])[first_client_name]\n",
    "    idx += 1\n",
    "    filename = \"confusion_matrix_\" + str(idx)\n",
    "    generate_confusion_matrix_only(y_true, y_pred, labels, path_to_exp_images,filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Summarizing the results over multiple Experiments (only for dataset 1 for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | Metric                           | Try 1    | Try 2    | Try 3     | Mean         | Std Dev     |\n",
    "# | -------------------------------- | -------- | -------- | --------- | ------------ | ----------- |\n",
    "# | Training Time (no compilation)   | 69.44 s  | 69.18 s  | 69.00 s   | **69.21 s**  | **0.22 s**  |\n",
    "# | Training Time (with compilation) | 334.40 s | 309.18 s | 280.00 s  | **307.86 s** | **27.36 s** |\n",
    "# | F1 Score                         | 0.77     | 0.79     | 0.74      | **0.77**     | **0.025**   |\n",
    "# | Accuracy                         | 0.79     | 0.81     | 0.80      | **0.80**     | **0.01**    |\n",
    "# | Charge Drop (Galaxy S10)         | 27 mAh   | 23 mAh   | 19.79 mAh | â€“            | â€“           |\n",
    "\n",
    "# Observations\n",
    "# Training Time (Without Compilation)\n",
    "\n",
    "# Very consistent across all runs (std dev: 0.22s), showing stable training overhead from the Android client.\n",
    "\n",
    "# This confirms the reliability of training_time as a metric for computation performance, excluding server overhead.\n",
    "\n",
    "# Training Time (With Compilation)\n",
    "\n",
    "# High variability (std dev: 27.36s), likely due to external usage of the laptop (e.g., background load during model recompilation).\n",
    "\n",
    "# Suggests compilation overhead should be interpreted with caution when the server environment is not idle.\n",
    "\n",
    "# Model Performance\n",
    "\n",
    "# F1 Score has moderate fluctuation (std dev: 0.025) but still reasonably consistent for early-stage FL experiments.\n",
    "\n",
    "# Accuracy is more stable (std dev: 0.01), reinforcing the modelâ€™s robustness across runs.\n",
    "\n",
    "# Energy Consumption\n",
    "\n",
    "# The charge drop varies, but without measurements from multiple devices or across more repetitions, itâ€™s difficult to draw meaningful conclusions.\n",
    "\n",
    "# Suggest excluding it from performance evaluation for now unless dedicated energy experiments are run.\n",
    "\n",
    "# although we did not interfere with the devices during the training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset 1 logs\n",
    "exp_name = \"dataset_1_iid_bal_5cl_20r_1\"\n",
    "df1_try_1 = stats_utils.parse_experiments_statistics_to_df(path_to_exp_statistics, exp_name, csv_filename=\"logs.csv\")\n",
    "\n",
    "# read dataset 2 logs\n",
    "exp_name = \"dataset_1_iid_bal_5cl_20r_2\"\n",
    "df1_try_2 = stats_utils.parse_experiments_statistics_to_df(path_to_exp_statistics, exp_name, csv_filename=\"logs.csv\")\n",
    "\n",
    "# read dataset 3 logs\n",
    "exp_name = \"dataset_1_iid_bal_5cl_20r_3\"\n",
    "df1_try_3 = stats_utils.parse_experiments_statistics_to_df(path_to_exp_statistics, exp_name, csv_filename=\"logs.csv\")\n",
    "\n",
    "dfs = [df1_try_1, df1_try_2, df1_try_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"dataset_1_variation\"\n",
    "plot_acc_loss_from_dfs(dfs, stats_utils.get_accuracy_loss_values_for_dfs, path_to_exp_images, should_save=True, filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric | dataset 1 | dataset 2 | dataset 3 | dataset 4 | dataset 5 | dataset 6\n",
    "\n",
    "# Training time (no compilation) | 334.4 +- 27.36 | 268.06 | 410.23 | 260.39 | 922.90 | 333.29\n",
    "# Training time (compilation) | 69.44 sec +- 0.22 | 71.16 | 143.42 | 76.65 | 673.62 | 116.55\n",
    "# F1 score (macro Avg) | 0.72 +- 0.025 | 0.88 | 0.99 | 0.86 | 0.52 | 0.5\n",
    "# accuracy | 0.79 +- 0.01 | 0.88 | 0.99 | 0.87 | 0.89 | 0.89\n",
    "\n",
    "# here i will present my observations\n",
    "# these are the comments made for dataset 1 which was run for 3 iters to get more statistically\n",
    "# significant results\n",
    "# Observations\n",
    "# Training Time (Without Compilation)\n",
    "\n",
    "# Very consistent across all runs (std dev: 0.22s), showing stable training overhead from the Android client.\n",
    "\n",
    "# This confirms the reliability of training_time as a metric for computation performance, excluding server overhead.\n",
    "\n",
    "# Training Time (With Compilation)\n",
    "\n",
    "# High variability (std dev: 27.36s), likely due to external usage of the laptop (e.g., background load during model recompilation).\n",
    "\n",
    "# Suggests compilation overhead should be interpreted with caution when the server environment is not idle.\n",
    "\n",
    "# Model Performance\n",
    "\n",
    "# F1 Score has moderate fluctuation (std dev: 0.025) but still reasonably consistent for early-stage FL experiments.\n",
    "\n",
    "# Accuracy is more stable (std dev: 0.01), reinforcing the modelâ€™s robustness across runs.\n",
    "\n",
    "# Energy Consumption\n",
    "\n",
    "\n",
    "# although we did not interfere with the devices during the training time\n",
    "\n",
    "# these are the comments for the performance of all the datasets as a whole\n",
    "# we can see all the 6 datasets get reasonably good accuracies values\n",
    "# and we can see the accuracy plot to have been stabilized by around the federated round 5\n",
    "# however alhouth datasets 5 and 6 have hihg accuracies scores this is not true for the f1-scores\n",
    "# (give the reason f1-score its better than the accuracy especially if we are talking about \n",
    "# imbalanced datasets for f1: When it's useful: When you care about false positives and false negatives, especially in imbalanced datasets or when the cost of misclassification is high.\n",
    "# for accuracy: When it's useful: When the classes are balanced (roughly equal number of examples in each class).\n",
    "# Limitation: In imbalanced datasets, accuracy can be misleading.)\n",
    "# we can see this more clearly on their consfusion matrices where on both cases\n",
    "# the model is heavily biased towards 'Walking' label\n",
    "# and on dataset5 labels {ascending stairs, descending stairs, clapping} is misclassified to walking\n",
    "# and on dataset6 labels {downstairs, upstairs, jogging} are also misclassified to walking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
